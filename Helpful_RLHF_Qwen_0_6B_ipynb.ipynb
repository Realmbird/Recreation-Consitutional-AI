{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Realmbird/Recreation-Consitutional-AI/blob/main/Helpful_RLHF_Qwen_0_6B_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets==4.0.0 transformers==4.54.1 trl==0.20.0 bitsandbytes==0.46.1 accelerate==1.9.0 peft==0.16.0 huggingface-hub==0.34.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F22xopwcvGHv",
        "outputId": "8d454886-6301-49b4-caee-07379a84c8c9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets==4.0.0 in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: transformers==4.54.1 in /usr/local/lib/python3.11/dist-packages (4.54.1)\n",
            "Requirement already satisfied: trl==0.20.0 in /usr/local/lib/python3.11/dist-packages (0.20.0)\n",
            "Requirement already satisfied: bitsandbytes==0.46.1 in /usr/local/lib/python3.11/dist-packages (0.46.1)\n",
            "Requirement already satisfied: accelerate==1.9.0 in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: peft==0.16.0 in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: huggingface-hub==0.34.1 in /usr/local/lib/python3.11/dist-packages (0.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==4.0.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==4.0.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==4.0.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==4.0.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==4.0.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==4.0.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==4.0.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==4.0.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==4.0.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==4.0.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==4.0.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.1) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.1) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.54.1) (0.5.3)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes==0.46.1) (2.6.0+cu124)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==1.9.0) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.34.1) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.34.1) (1.1.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (3.12.14)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==4.0.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==4.0.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==4.0.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==4.0.0) (2025.7.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes==0.46.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes==0.46.1) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==4.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==4.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==4.0.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==4.0.0) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes==0.46.1) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, AutoModelForCausalLM, pipeline\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "model_name = \"Qwen/Qwen3-0.6B\" # Or any other suitable model\n",
        "\n",
        "mname = model_name\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "dnfz_uPBvSyc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"Anthropic/hh-rlhf\", data_dir=\"helpful-base\")\n",
        "\n",
        "\n",
        "def get_prompts(dataset):\n",
        "\n",
        "    prompts = []\n",
        "\n",
        "    for example in dataset:\n",
        "\n",
        "        chosen_text = example['chosen']\n",
        "\n",
        "        # Find the end of the human's prompt (before \"Assistant:\")\n",
        "\n",
        "        prompt_end_idx = chosen_text.find(\"\\n\\nAssistant:\")\n",
        "\n",
        "        if prompt_end_idx != -1:\n",
        "\n",
        "            prompts.append(chosen_text[:prompt_end_idx].strip())\n",
        "\n",
        "    return prompts\n",
        "\n",
        "\n",
        "# prompts had to formated of just queries\n",
        "\n",
        "test_prompts_list = get_prompts(dataset[\"train\"]) # example just used train not test for promptlist\n",
        "\n",
        "eval_prompts_list = get_prompts(dataset[\"test\"])\n",
        "\n",
        "\n",
        "\n",
        "#dataset that ppotrainer accepts is tokens\n",
        "\n",
        "def tokenize(sample):\n",
        "\n",
        "    sample[\"input_ids\"] = tokenizer.encode(sample)\n",
        "\n",
        "    return sample\n",
        "\n",
        "\n",
        "\n",
        "test_dataset = dataset.map(tokenize, batched=False)\n",
        "\n",
        "eval_dataset = dataset.map(tokenize, batched=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "0utnHSIHvJ6P"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "\n",
        "    load_in_4bit=True,   # ✅ Use 4-bit instead\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "# Important: Add a pad token if the tokenizer doesn't have one, especially for decoder models.\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "\n",
        "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1, pad_token_id=tokenizer.pad_token_id, quantization_config=bnb_config) # accepted and rejceted label\n",
        "\n",
        "# Resize token embeddings if you added a new pad token\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "\n",
        "    model_name,\n",
        "\n",
        "    quantization_config=bnb_config,\n",
        "\n",
        "    device_map=\"auto\"\n",
        "\n",
        ")\n",
        "\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(\n",
        "\n",
        "    model_name,\n",
        "\n",
        "    quantization_config=bnb_config,\n",
        "\n",
        "    device_map=\"auto\"\n",
        "\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie3FGyZGvalL",
        "outputId": "e38b53d7-071e-40e1-ed2d-100320638044"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-0.6B and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, TaskType, PeftModel, get_peft_model\n",
        "\n",
        "\n",
        "# stacks on top of BitsAndBytes\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "\n",
        "    inference_mode=False,\n",
        "\n",
        "    r=8,\n",
        "\n",
        "    lora_alpha=32,\n",
        "\n",
        "    lora_dropout=0.1,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "_U6aYvb4vlqM"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl.trainer.utils import disable_dropout_in_model\n",
        "\n",
        "from trl import PPOTrainer, PPOConfig\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "\n",
        "# Load the reward model directly\n",
        "\n",
        "reward_model_name = \"Realmbird/helpfulness-preference-model-qwen-0.6B-v2\"\n",
        "\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_name, num_labels=1, device_map=\"auto\", torch_dtype=torch.float16) # Added device_map=\"auto\"\n",
        "\n"
      ],
      "metadata": {
        "id": "aSfEaBhKvpiq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "\n",
        "    model_name,\n",
        "\n",
        "    num_labels=1,\n",
        "\n",
        "    quantization_config=bnb_config,\n",
        "\n",
        "    device_map=\"auto\"\n",
        "\n",
        ")\n",
        "\n",
        "value_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "\n",
        "# Apply PEFT to the value model as well\n",
        "\n",
        "peft_config_value = LoraConfig(\n",
        "\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "\n",
        "    inference_mode=False,\n",
        "\n",
        "    r=8,\n",
        "\n",
        "    lora_alpha=32,\n",
        "\n",
        "    lora_dropout=0.1,\n",
        "\n",
        ")\n",
        "\n",
        "value_model = get_peft_model(value_model, peft_config_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpkwbqpivzhI",
        "outputId": "ab2b115d-4100-4737-f580-887293cbbe84"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-0.6B and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "ppo_config = PPOConfig(\n",
        "\n",
        "    output_dir= \"./rlhf_helpful_Qwen3_0.6B\",\n",
        "\n",
        "    learning_rate=1.41e-5,\n",
        "\n",
        "    batch_size=4,\n",
        "\n",
        "    mini_batch_size=1,\n",
        "\n",
        "    gradient_accumulation_steps=4,\n",
        "\n",
        ")\n",
        "\n",
        "trainer = PPOTrainer(\n",
        "\n",
        "    args=ppo_config,\n",
        "\n",
        "    model=model,\n",
        "\n",
        "    ref_model = ref_model,\n",
        "\n",
        "    processing_class=tokenizer,\n",
        "\n",
        "    train_dataset=test_dataset, # Use the training split\n",
        "\n",
        "    eval_dataset=eval_dataset,\n",
        "\n",
        "    peft_config=peft_config,\n",
        "\n",
        "    reward_model=reward_model,\n",
        "\n",
        "    value_model= value_model\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "siMYyQnYv2ZJ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import trl\n",
        "\n",
        "print(trl.__version__)"
      ],
      "metadata": {
        "id": "k67HpBXnvdNG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNbTvfFYjibPnorJP/csgXS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}