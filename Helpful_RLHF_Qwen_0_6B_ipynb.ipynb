{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Realmbird/Recreation-Consitutional-AI/blob/main/Helpful_RLHF_Qwen_0_6B_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets==4.0.0 transformers==4.54.1 trl==0.20.0 bitsandbytes==0.46.1 accelerate==1.9.0 peft==0.16.0 huggingface-hub==0.34.1"
      ],
      "metadata": {
        "id": "F22xopwcvGHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, AutoModelForCausalLM, pipeline\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "model_name = \"Qwen/Qwen3-0.6B\" # Or any other suitable model\n",
        "\n",
        "mname = model_name\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Important: Add a pad token if the tokenizer doesn't have one, especially for decoder models.\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "\n",
        "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
      ],
      "metadata": {
        "id": "dnfz_uPBvSyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prompts(dataset):\n",
        "\n",
        "\n",
        "    prompts = []\n",
        "\n",
        "\n",
        "    for example in dataset:\n",
        "\n",
        "\n",
        "        chosen_text = example['chosen']\n",
        "\n",
        "\n",
        "        # Find the end of the human's prompt (before \"Assistant:\")\n",
        "\n",
        "\n",
        "        prompt_end_idx = chosen_text.find(\"\\n\\nAssistant:\")\n",
        "\n",
        "\n",
        "        if prompt_end_idx != -1:\n",
        "\n",
        "\n",
        "            prompts.append(chosen_text[:prompt_end_idx].strip())\n",
        "\n",
        "\n",
        "    return prompts"
      ],
      "metadata": {
        "id": "wMAgYZ_yK12P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"Anthropic/hh-rlhf\", data_dir=\"helpful-base\")\n",
        "\n"
      ],
      "metadata": {
        "id": "0utnHSIHvJ6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_prompts(prompts_list, tokenizer):\n",
        "    tokenized_data = []\n",
        "    for prompt in prompts_list:\n",
        "        # TRL's PPOTrainer often expects \"input_ids\" and \"attention_mask\"\n",
        "        # for the queries.\n",
        "        tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        tokenized_data.append({\n",
        "            \"input_ids\": tokenized_prompt[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": tokenized_prompt[\"attention_mask\"].squeeze(0)\n",
        "        })\n",
        "    return Dataset.from_list(tokenized_data) # Convert list of dicts to a datasets.Dataset"
      ],
      "metadata": {
        "id": "_DS1dkyiLf46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompts had to formated of just queries\n",
        "train_prompts_list = get_prompts(dataset[\"train\"])\n",
        "eval_prompts_list = get_prompts(dataset[\"test\"])\n",
        "\n",
        "train_dataset = tokenize_prompts(train_prompts_list, tokenizer)\n",
        "eval_dataset = tokenize_prompts(eval_prompts_list, tokenizer)"
      ],
      "metadata": {
        "id": "19ELpyPiLcTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,   # âœ… Use 4-bit instead\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "\n",
        "    model_name,\n",
        "\n",
        "    quantization_config=bnb_config,\n",
        "\n",
        "    device_map=\"auto\"\n",
        "\n",
        ")\n",
        "\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(\n",
        "\n",
        "    model_name,\n",
        "\n",
        "    quantization_config=bnb_config,\n",
        "\n",
        "    device_map=\"auto\"\n",
        "\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Ie3FGyZGvalL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, TaskType, PeftModel, get_peft_model\n",
        "\n",
        "\n",
        "# stacks on top of BitsAndBytes"
      ],
      "metadata": {
        "id": "_U6aYvb4vlqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl.trainer.utils import disable_dropout_in_model\n",
        "\n",
        "from trl import PPOTrainer, PPOConfig\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "\n",
        "# Load the reward model directly\n",
        "\n",
        "reward_model_name = \"Realmbird/helpfulness-preference-model-qwen-0.6B-merged\"\n",
        "\n",
        "\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_name, num_labels=1, device_map=\"auto\", torch_dtype=torch.float16) # Added device_map=\"auto\"\n",
        "\n"
      ],
      "metadata": {
        "id": "aSfEaBhKvpiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_model"
      ],
      "metadata": {
        "id": "o2F00Jp7xV1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "\n",
        "    model_name,\n",
        "\n",
        "    num_labels=1,\n",
        "\n",
        "    quantization_config=bnb_config,\n",
        "\n",
        "    device_map=\"auto\"\n",
        "\n",
        ")\n",
        "\n",
        "value_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "\n",
        "# Apply PEFT to the value model as well\n",
        "\n",
        "peft_config_value = LoraConfig(\n",
        "\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "\n",
        "    inference_mode=False,\n",
        "\n",
        "    r=8,\n",
        "\n",
        "    lora_alpha=32,\n",
        "\n",
        "    lora_dropout=0.1,\n",
        "\n",
        ")\n",
        "\n",
        "value_model = get_peft_model(value_model, peft_config_value)"
      ],
      "metadata": {
        "id": "bpkwbqpivzhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- NEW: Set pad_token_id on the model's config ---\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "ref_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "value_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "reward_model.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "CSmhMDURQg16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "peft_config_policy = LoraConfig( # Renamed for clarity\n",
        "    task_type=TaskType.CAUSAL_LM, # <--- This MUST be CAUSAL_LM for your generator!\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        ")\n",
        "\n",
        "ppo_config = PPOConfig(\n",
        "\n",
        "    output_dir= \"./rlhf_helpful_Qwen3_0.6B\",\n",
        "\n",
        "    learning_rate=1.41e-5,\n",
        "\n",
        "    num_train_epochs=1,\n",
        "\n",
        "    batch_size=4,\n",
        "\n",
        "    mini_batch_size=1,\n",
        "\n",
        "    gradient_accumulation_steps=32,\n",
        "\n",
        "    report_to=\"none\"\n",
        "\n",
        ")\n",
        "\n",
        "trainer = PPOTrainer(\n",
        "\n",
        "    args=ppo_config,\n",
        "\n",
        "    model=model,\n",
        "\n",
        "    ref_model = False,\n",
        "\n",
        "    processing_class=tokenizer,\n",
        "\n",
        "    train_dataset=train_dataset, # Use the training split\n",
        "\n",
        "    eval_dataset=eval_dataset,\n",
        "\n",
        "    peft_config=peft_config_policy,\n",
        "\n",
        "    reward_model=reward_model,\n",
        "\n",
        "    value_model= value_model,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "siMYyQnYv2ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16): # or torch.bfloat16\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "q2EH5JjuOhID"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOjhx9JC3A7wZnu+EdWy+CX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}